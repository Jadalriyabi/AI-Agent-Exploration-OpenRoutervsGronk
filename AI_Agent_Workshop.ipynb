{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO8gHDbSAa4H"
      },
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter AI Agent Workshop\n",
        "\n",
        "#### **Skills: OpenAI, Groq, Llama, OpenRouter**\n",
        "\n",
        "## **To Get Started:**\n",
        "1. [Get your Groq API Key](https://console.groq.com/keys)\n",
        "2. [Get your OpenRouter API Key](https://openrouter.ai/settings/keys)\n",
        "3. [Get your OpenAI API Key](https://platform.openai.com/api-keys)\n",
        "\n",
        "\n",
        "### **Interesting Reads**\n",
        "- [Sam Altman's Blog Post: The Intelligence Age](https://ia.samaltman.com/)\n",
        "- [What LLMs cannot do](https://ehudreiter.com/2023/12/11/what-llms-cannot-do/)\n",
        "- [Chain of Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n",
        "- [Why ChatGPT can't count the number of r's in the word strawberry](https://prompt.16x.engineer/blog/why-chatgpt-cant-count-rs-in-strawberry)\n",
        "\n",
        "\n",
        "## During the Workshop\n",
        "- [Any code shared during the workshop will be posted here](https://docs.google.com/document/d/1hPBJt_4Ihkj6v667fWxVjzwCMS4uBPdYlBLd2IqkxJ0/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt_SMGlvocqA"
      },
      "source": [
        "# Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKvKwO8XAnPt",
        "outputId": "44bb578b-7914-40eb-e817-a56b174f9adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.50.2-py3-none-any.whl (382 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.0/383.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai, groq\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.50.2\n"
          ]
        }
      ],
      "source": [
        "! pip install openai groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjcgEeFaof1i"
      },
      "source": [
        "# Set up Groq, OpenRouter, & OpenAI clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pHbjDU_L__Vd"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "from typing import List, Dict, Any, Callable\n",
        "import ast\n",
        "import io\n",
        "import sys\n",
        "\n",
        "# Retrieve API keys and set environment variables\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "openrouter_api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "os.environ['OPENROUTER_API_KEY'] = openrouter_api_key\n",
        "\n",
        "# Initialize clients for Groq and OpenRouter\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZF6uLpooj-F"
      },
      "source": [
        "### Define functions to easily query and compare responses from OpenAI, Groq, and OpenRouter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BNYyDCNuAhbj"
      },
      "outputs": [],
      "source": [
        "# Function to get LLM responses from Groq or OpenRouter\n",
        "def get_llm_response(client, prompt, json_mode=False):\n",
        "    if client == \"groq\":\n",
        "        models = [\n",
        "            \"llama-3.1-8b-instant\",\n",
        "            \"llama-3.1-70b-versatile\",\n",
        "            \"llama3-70b-8192\",\n",
        "            \"llama3-8b-8192\",\n",
        "            \"gemma2-9b-it\"\n",
        "        ]\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                kwargs = {\n",
        "                    \"model\": model,\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "                }\n",
        "                if json_mode:\n",
        "                    kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "                response = groq_client.chat.completions.create(**kwargs)\n",
        "                return response.choices[0].message.content\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with Groq model {model}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(\"All Groq models failed to provide a response.\")\n",
        "        return None\n",
        "\n",
        "    elif client == \"openrouter\":\n",
        "        models = [\"meta-llama/llama-3.1-8b-instruct:free\"]\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                kwargs = {\n",
        "                    \"model\": model,\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "                }\n",
        "                if json_mode:\n",
        "                    kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "                response = openrouter_client.chat.completions.create(**kwargs)\n",
        "                return response.choices[0].message.content\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with OpenRouter model {model}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(\"OpenRouter failed to provide a response.\")\n",
        "        return None\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid client: {client}\")\n",
        "\n",
        "# Function to evaluate and compare responses from Groq and OpenRouter\n",
        "def evaluate_responses(prompt, reasoning_prompt=False):\n",
        "    if reasoning_prompt:\n",
        "        prompt = f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
        "\n",
        "    groq_response = get_llm_response(\"groq\", prompt)\n",
        "    openrouter_response = get_llm_response(\"openrouter\", prompt)\n",
        "\n",
        "    print(f\"Groq Response: {groq_response}\")\n",
        "    print(f\"\\n\\nOpenRouter Response: {openrouter_response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9V03RXNBd6j",
        "outputId": "ca7f74b3-6e5c-4886-8c5c-dd295e4041c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq Response: There are 2 r's in the word 'strawberry'.\n",
            "\n",
            "\n",
            "OpenRouter Response: There are 2 r's in the word 'strawberry'.\n"
          ]
        }
      ],
      "source": [
        "prompt1 = \"How many r's are in the word 'strawberry'?\"\n",
        "evaluate_responses(prompt1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgyXx8M9C0RT",
        "outputId": "08bf18cc-a9ef-4a00-99ee-b09f58cb9f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq Response: Both numbers are very close. However, 9.11 is a decimal greater than 9.9.\n",
            "\n",
            "\n",
            "OpenRouter Response: The answer is... 9.11 is bigger.\n",
            "\n",
            "To see why, let's compare the two numbers:\n",
            "\n",
            "9.9 and 9.11 are both decimal numbers. When we compare them, we can see that 9.11 is slightly bigger than 9.9 because the second number has a hundredth (0.11) that is greater than the second number of the first number (0.9).\n",
            "\n",
            "In other words, 9.11 is bigger by 0.02, or two tenths of a decimal place.\n"
          ]
        }
      ],
      "source": [
        "prompt2 = \"Whats bigger 9.9 or 9.11'?\"\n",
        "evaluate_responses(prompt2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w8IlZq49PAz"
      },
      "source": [
        "# Agent Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxpUp2KED9hh"
      },
      "source": [
        "[![](https://mermaid.ink/img/pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW?type=png)](https://mermaid.live/edit#pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW)\n",
        "\n",
        "\n",
        "![agent_architecture_v2](https://github.com/user-attachments/assets/a65b6db9-bef1-4579-aed3-01444ce40544)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGb1KUVmVu1"
      },
      "source": [
        "### To create our AI Agent, we will define the following functions:\n",
        "\n",
        "1. **Planner:** This function takes a user's query and breaks it down into smaller, manageable subtasks. It returns these subtasks as a list, where each one is either a reasoning task or a code generation task.\n",
        "\n",
        "2. **Reasoner:** This function provides reasoning on how to complete a specific subtask, considering both the overall query and the results of any previous subtasks. It returns a short explanation on how to proceed with the current subtask.\n",
        "\n",
        "3. **Actioner:** Based on the reasoning provided for a subtask, this function decides whether the next step requires generating code or more reasoning. It then returns the chosen action and any necessary details to perform it.\n",
        "\n",
        "4. **Evaluator:** This function checks if the result of the current subtask is reasonable and aligns with the overall goal. It returns an evaluation of the result and indicates whether the subtask needs to be retried.\n",
        "\n",
        "5. **generate_and_execute_code:** This function generates and executes Python code based on a given prompt and memory of previous steps. It returns both the generated code and its execution result.\n",
        "\n",
        "6. **executor:** Depending on the action decided by the \"actioner,\" this function either generates and executes code or returns reasoning. It handles the execution of tasks based on the action type.\n",
        "\n",
        "7. **final_answer_extractor:** After all subtasks are completed, this function gathers the results from previous steps to extract and provide the final answer to the user's query.\n",
        "\n",
        "8. **autonomous_agent:** This is the main function that coordinates the process of answering the user's query. It manages the entire sequence of planning, reasoning, action, evaluation, and final answer extraction to produce a complete response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BpZ1hRaiD-hS"
      },
      "outputs": [],
      "source": [
        "def planner(user_query) -> List[str]:\n",
        "  prompt = f\"\"\"Given the user's query: '{user_query}', break down the query into as few subtasks as possibile in orde to answer the question.\n",
        "\n",
        "  Each subtask should be either a reasonsing task or a code generation task. Never duplicate a task.\n",
        "\n",
        "  Here are the only 2 actions taht can be taken for each subtask:\n",
        "    - generater_code: This actions involves generating Python code and execting it inorder to make a calcualtion or a vaerfication\n",
        "    - reasosnign: This asction invovles proving reasosnign for waht to do to complete the subtask\n",
        "\n",
        "  Each subtask should begin with either \"reasosning\" or \"generate_code\".\n",
        "\n",
        "  Keep in mind the overall goal of answeing the useers query thoughout the planning process\n",
        "\n",
        "  Return the result as a JSON list of strings, where each string is a subtask.\n",
        "\n",
        "  Here is an example JSON repsosne:\n",
        "  {{\n",
        "      \"subtasks\": [\"Subtask 1\", \"Subtask 2\", \"Subtask 3\"]\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  response = json.loads(get_llm_response(\"openrouter\", prompt, json_mode=True))\n",
        "  return response[\"subtasks\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUMjfWwcK7ig",
        "outputId": "e7b6b74d-d6b2-4efb-c6fe-82589c100073"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Reasoning: Identify the word in the user's query to count the number of r's\",\n",
              " \"Reasoning: Determine the correct count of r's in the word 'strawberry'\",\n",
              " \"Generate_code: Write a Python function to count the number of 'r's in the word 'strawberry'\"]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"How many r's are in the word 'strawberry'?\"\n",
        "subtasks = planner(query)\n",
        "subtasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BWjY2S2mxh3w"
      },
      "outputs": [],
      "source": [
        "def reasoner(user_query: str, subtasks: List[str], current_subtask: str, memory: List[Dict[str, Any]]) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   Here are all the subtasks to complete in order to answer the user's query:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   - Provide concise reasoning on how to execute the current subtask, considering previous results.\n",
        "   - Prioritize explicit details over assumed patterns\n",
        "   - Avoid unnecessary complications in problem-solving\n",
        "\n",
        "   Return the result as a JSON object with 'reasoning' as a key.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"reasoning\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PfLTeLJS2X9K",
        "outputId": "87915795-301b-47cc-d18a-f3f8e1ba31b1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Parse the user's query for the specific word, in this case 'strawberry', considering it as a string. Use a text extraction technique, such as tokenization or regular expressions, to isolate the word of interest and make it the target for counting 'r's.\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reasoner_ouput = reasoner(query, subtasks, subtasks[0], [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1BFyiCpRxh6I"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The reasoning for this subtask is:\n",
        "   <reasoning>\n",
        "       {reasoning}\n",
        "   </reasoning>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   Determine the most appropriate action to take:\n",
        "       - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
        "       - If the task requires reasoning without code or calculations, use the 'reasoning' action.\n",
        "\n",
        "   Consider the overall goal and previous results when determining the action.\n",
        "\n",
        "   Return the result as a JSON object with 'action' and 'parameters' keys.  The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
        "\n",
        "   Example JSON responses:\n",
        "\n",
        "   {{\n",
        "       \"action\": \"generate_code\",\n",
        "       \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
        "   }}\n",
        "\n",
        "   {{\n",
        "       \"action\": \"reasoning\",\n",
        "       \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_gBHITk2n7u",
        "outputId": "ed57a6bb-b4a7-48c9-ecea-a95b078669d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'action': 'reasoning',\n",
              " 'parameters': {'prompt': \"The word 'strawberry' contains two 'r's in a sequence, one more single 'r'. Now we need to count the individual 'r's which are 3.\"}}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actioner(query, subtasks, subtasks[1], reasoner_ouput, []) # Added an empty list as the memory argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NIDY6vbt0yaX"
      },
      "outputs": [],
      "source": [
        "def evaluator(user_query: str, subtasks: List[str], current_subtask: str, action_info: Dict[str, Any], execution_result: Dict[str, Any], memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks to complete to answer the user's query are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The result of the current subtask is:\n",
        "   <result>\n",
        "       {action_info}\n",
        "   </result>\n",
        "\n",
        "   The execution result of the current subtask is:\n",
        "   <execution_result>\n",
        "       {execution_result}\n",
        "   </execution_result>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "\n",
        "   Evaluate if the result is a reasonable answer for the current subtask, and makes sense in the context of the overall query.\n",
        "\n",
        "   Return a JSON object with 'evaluation' (string) and 'retry' (boolean) keys.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"evaluation\": \"The result is a reasonable answer for the current subtask.\",\n",
        "       \"retry\": false\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KySBKdp3ILl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lX1pObiE01nF"
      },
      "outputs": [],
      "source": [
        "def final_answer_extractor(user_query: str, subtasks: List[str], memory: List[Dict[str, Any]]) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks completed to answer the user's query are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The memory of the thought process (short-term memory) is:\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   Extract the final answer that directly addresses the user's query, from the memory.\n",
        "   Provide only the essential information without unnecessary explanations.\n",
        "\n",
        "   Return a JSON object with 'finalAnswer' as a key.\n",
        "\n",
        "   Here is an example JSON response:\n",
        "   {{\n",
        "       \"finalAnswer\": \"The final answer to the user's query, addressing all aspects of the question, based on the memory provided\",\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"finalAnswer\"]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wAl8-7yu06SU"
      },
      "outputs": [],
      "source": [
        "def generate_and_execute_code(prompt: str, user_query: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   code_generation_prompt = f\"\"\"\n",
        "\n",
        "   Generate Python code to implement the following task: '{prompt}'\n",
        "\n",
        "   Here is the overall goal of answering the user's query: '{user_query}'\n",
        "\n",
        "   Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "\n",
        "\n",
        "   Here are the guidelines for generating the code:\n",
        "       - Return only the Python code, without any explanations or markdown formatting.\n",
        "       - The code should always print or return a value\n",
        "       - Don't include any backticks or code blocks in your response. Do not include in your response, just give me the code.\n",
        "       - Do not ever use the input() function in your code, use defined values instead.\n",
        "       - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
        "       - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
        "       - Don't ever provide the execution result in your response, just give me the code.\n",
        "       - If your code needs to import any libraries, do it within the code itself.\n",
        "       - The code should be self-contained and ready to execute on its own.\n",
        "       - Prioritize explicit details over assumed patterns\n",
        "       - Avoid unnecessary complications in problem-solving\n",
        "   \"\"\"\n",
        "\n",
        "   generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "\n",
        "   print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "   old_stdout = sys.stdout\n",
        "   sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "   # Add import statement for the 're' module before executing the generated code\n",
        "   exec('import re\\n' + generated_code)\n",
        "\n",
        "   sys.stdout = old_stdout\n",
        "   output = buffer.getvalue()\n",
        "\n",
        "   print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "   return {\n",
        "       \"generated_code\": generated_code,\n",
        "       \"execution_result\": output.strip()\n",
        "   }\n",
        "\n",
        "   def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
        "    if action == \"generate_code\":\n",
        "        print(f\"Generating code for: {parameters['prompt']}\")\n",
        "        return generate_and_execute_code(parameters[\"prompt\"], user_query, memory)\n",
        "    elif action == \"reasoning\":\n",
        "        return parameters[\"prompt\"]\n",
        "    else:\n",
        "        return f\"Action '{action}' not implemented\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6L-jxKs509_L"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "gsGZyWOk1Aoz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def autonomous_agent(user_query: str) -> List[Dict[str, Any]]:\n",
        "   memory = []\n",
        "   subtasks = planner(user_query)\n",
        "\n",
        "   print(\"User Query:\", user_query)\n",
        "   print(f\"Subtasks: {subtasks}\")\n",
        "\n",
        "   for subtask in subtasks:\n",
        "       max_retries = 1\n",
        "       for attempt in range(max_retries):\n",
        "\n",
        "           reasoning = reasoner(user_query, subtasks, subtask, memory)\n",
        "           action_info = actioner(user_query, subtasks, subtask, reasoning, memory)\n",
        "\n",
        "\n",
        "           print(f\"\\n\\n ****** Action Info: {action_info} ****** \\n\\n\")\n",
        "\n",
        "           execution_result = executor(action_info[\"action\"], action_info[\"parameters\"], user_query, memory)\n",
        "\n",
        "           print(f\"\\n\\n ****** Execution Result: {execution_result} ****** \\n\\n\")\n",
        "           evaluation = evaluator(user_query, subtasks, subtask, action_info, execution_result, memory)\n",
        "\n",
        "           step = {\n",
        "               \"subtask\": subtask,\n",
        "               \"reasoning\": reasoning,\n",
        "               \"action\": action_info,\n",
        "               \"evaluation\": evaluation\n",
        "           }\n",
        "           memory.append(step)\n",
        "\n",
        "           print(f\"\\n\\nSTEP: {step}\\n\\n\")\n",
        "\n",
        "           if not evaluation[\"retry\"]:\n",
        "               break\n",
        "\n",
        "           if attempt == max_retries - 1:\n",
        "               print(f\"Max retries reached for subtask: {subtask}\")\n",
        "\n",
        "   final_answer = final_answer_extractor(user_query, subtasks, memory)\n",
        "   return final_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "modHsD_G3vt0"
      },
      "outputs": [],
      "source": [
        "result = autonomous_agent(query)\n",
        "print(\"FiNAL ANSWER: \", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrSeaoPqq1qY"
      },
      "source": [
        "# OpenAI o1-preview model getting trivial questions wrong\n",
        "\n",
        "- [Link to Reddit post](https://www.reddit.com/r/ChatGPT/comments/1ff9w7y/new_o1_still_fails_miserably_at_trivial_questions/)\n",
        "- Links to ChatGPT threads: [(1)](https://chatgpt.com/share/66f21757-db2c-8012-8b0a-11224aed0c29), [(2)](https://chatgpt.com/share/66e3c1e5-ae00-8007-8820-fee9eb61eae5)\n",
        "- [Improving reasoning in LLMs through thoughtful prompting](https://www.reddit.com/r/singularity/comments/1fdhs2m/did_i_just_fix_the_data_overfitting_problem_in/?share_id=6DsDLJUu1qEx_bsqFDC8a&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM4bcny1JliP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXAPpgkpK7lG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjKgUDihI4lA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JNPSaGWKF5s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ciZ_VUNNJRh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vf9avjo8-fK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
